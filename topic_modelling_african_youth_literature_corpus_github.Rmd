---
title: "Topic modelling with a corpus of Africa youth literature"
author: "Bronwyn Bowles-King"
date: "2025-04-06"
output:
  html_document: default
---

#### **STEP 0A: PREPARATION - SET WORKING DIRECTORY (wd), INSTALL AND LOAD LIBRARIES** {style="color: green"}

```{r}
knitr::opts_knit$set(root.dir = "SET WORKING DIRECTORY HERE") # Set your main working directory
```

**Install libraries if necessary (in the console)**

```{r}
# install.packages(c("stringi", "tidyverse", "tidytext", "textstem", "tidyr", "topicmodels"))
```

**Load libraries and package versions**

```{r}
library(stringi)
library(tidyverse)
library(tidytext)
library(textstem) 
packageVersion("textstem")
library(tidyr)
packageVersion("tidyr")
library(topicmodels)
```

#### STEP 0B: LOAD TEXT FILES {style="color: green"}

```{r}
file_dir <- "SET FILE DIRECTORY HERE" # Set the directory for the plain text files

files <- list.files(file_dir, pattern = "\\.txt$", full.names = TRUE) # Load all .txt files into file_dir
```

#### STEP 1A: CLEAN THE TEXT {style="color: green"}

```{r}
clean_text <- function(text) { # Create cleaning function
text <- iconv(text, from = "UTF-8", to = "UTF-8", sub = "") # Ensure UTF-8 is properly encoded and invalid characters are removed
text <- gsub("[\n\r\t]", " ", text) # Replace white space with a space
return(text)
}

process_files <- function(file) { # Create helper (looping) function 
message(paste("Processing:", file))
text <- read_file(file)
cleaned_text <- clean_text(text) # Use cleaning function created above
write_file(cleaned_text, file) # Save cleaned .txt file with the same file name
}

lapply(files, process_files) # Run helper function (looping) on all files

files <- list.files(file_dir, pattern = "\\.txt$", full.names = TRUE) # Overwrite with cleaned files 
length(files)

texts <- tibble(doc_id = basename(files), text = sapply(files, read_file)) # Create clean table (tibble) with file names as IDs and cleaned data
texts
```

#### STEP 1B: TOKENISE THE TEXT {style="color: green"}

```{r}
tokens <- texts %>%
unnest_tokens(word, text)
View(tokens)
dim(tokens)
```

#### STEP 2A: CREATE A TERM FILTER LIST AND KEEP WORDS {style="color: green"}

**Term filter list**

-   A goal of the quantitative component of the research is to get a sense of the overall thematic structure of the corpus. Various iterations of topic modelling can and have been run for the corpus with and without lemmatising and removing filter terms and stopwords to this end.

-   The term `filter_list` created below includes common words in the corpus that do not indicate theme or topic and skew the results towards words such as character names. This leads to 'undercooked' or 'overcooked' topics with poor interpretability (Van Kessel, 2019).

-   It was found that removing stopwords, lemmatising the text and using a term `filter_list` best achieved the goal of arriving at useful sets of terms.

-   The term `filter_list` is created below.

```{r}
filter_list <- c(
  
  # i. Character names, dialogue tags and stage directions
  "act", "addie", "ahasuerus", "alex", "angie", "barnes", "beat", "bill", "bon", 
  "cape", "chamberlain", "chamberlains", "chuka", "cicilia", "cj", "claudius", 
  "clynn", "cortina", "cyra", "danny", "dede", "dipalesa", "dougie", "easton", 
  "edin", "edins", "enter", "eunuch", "eunuchs", "ewa", "exit", "folsade", 
  "farzad", "gertrude", "hamlet", "herod", "herods", "hussein", "irsia", "janny", 
  "jenesa", "johnny", "joke", "jules", "karina", "knoxman", "latiff", "leslie", 
  "lina", "lina's", "luke", "lulu", "lunga", "mariah", "martha", "mbali", "melody",   "memucan", "mia", "midas", "mike", "minister", "moramang", "narrator", "noble", 
  "nobles", "nkechi", "nongqawuse", "nshuti", "ntate", "ntokozo", "nyasha", "oga", 
  "omolara", "ophelia", "operbia", "policeman", "rhulani", "roseline", "sabelo", 
  "samukelo", "scene", "shawn", "simon", "soso", "sphe", "sphiwe", "stache", 
  "tanya", "themba", "thembas", "tyson", "vashti", "willem", "willems", "xabisa", 
  "xina", "yenzi", "zika",
  
  # ii. Common adjectives and adverbs
  "accurately", "actively", "admittedly", "almost", "already", "alternately", 
  "always", "artificially", "automatically", "back", "bad", "barely", "big", 
  "boldly", "calmly", "carelessly", "cautiously", "certainly", "cheerfully", 
  "clearly", "cleverly", "closely", "coincidentally", "commonly", "complete", 
  "completely", "consistently", "conveniently", "cooperatively", "correctly", 
  "creatively", "deeply", "deliberately", "desperately", "dramatically", 
  "easily", "efficiently", "energetically", "enthusiastically", "exclusively", 
  "extreme", "extremely", "fairly", "fast", "finally", "freely", "fully", 
  "generally", "good", "hardly", "highly", "incredibly", "incomplete", 
  "independently", "instantly", "intensely", "just", "justly", "kindly", 
  "largely", "lot", "lots", "loosely", "loudly", "mainly", "many", 
  "merely", "mildly", "most", "mostly", "much", "naturally", "nearly", "no", 
  "normal", "normally", "obviously", "odd", "oddly", "often", "only", 
  "originally", "partially", "perfectly", "personally", "pleasantly", "poorly", 
  "powerfully", "quiet", "quietly", "quick", "quickly", "quite", "rather", 
  "rarely", "readily", "really", "remarkably", "rightly", "roughly", "sadly", 
  "safely", "scarcely", "seriously", "sharply", "short", "similarly", "simply", 
  "slightly", "slow", "slowly", "small", "smoothly", "softly", "solely", 
  "sometimes", "soon", "speedily", "strongly", "suddenly", "swiftly", "tightly", 
  "too", "total", "totally", "typical", "typically", "unjustly", "unusual", 
  "unusually", "usual", "utterly", "vaguely", "very", "vigorously", "violently", 
  "virtually", "visibly", "warmly", "weakly", "well", "wisely", "wonderfully", 
  "wrongly", "yes", 
  
  # iii. Conjunctions
  "accordingly", "addition", "additionally", "after", "again", "along", 
  "also", "although", "and", "anyhow", "anyway", "because", "before", 
  "besides", "both", "but", "consequently", "conversely", "either", "equally", 
  "even", "furthermore", "hence", "however", "if", "inasmuch", "indeed", 
  "instead", "lest", "likewise", "moreover", "namely", "neither", 
  "nevertheless", "nonetheless", "nor", "once", "or", "otherwise", "provided", 
  "since", "so", "still", "than", "that", "thereafter", "therefore", "though", 
  "thus", "till", "unless", "until", "when", "where", "whereby", "wherefore", 
  "whereupon", "whereas", "while", "whilst", "yet", 
  
  # iv. Determiners, contractions and possessives
  "a", "all", "an", "any", "couldnt", "didnt", "doesnt", "dont", "double", 
  "each", "enough", "every", "half", "hed", "he'll", "Her", "hers", "his", 
  "His", "i'd", "Id", "im", "i'm", "its", "it's", "Ive", "ive", "last", 
  "little", "many", "mine", "much", "my", "next", "other", "our", "Our", "ours", 
  "shed", "shell", "Shell", "some", "several", "somewhat", "such", "that", 
  "the", "their", "theirs", "these", "this", "those", "we", "We", "wed", "were", 
  "what", "whatever", "whose", "youd", "you'd", "youll", "you'll", "your", 
  "Your", "youre", "you're", "yours", "youve", "you've", 
  
  # v. Filler words 
  "ah", "bit", "bits", "dey", "ein", "guy", "hey", "hi", "la", "le", "na", "ne", 
  "oh", "um", "wey", "ye", "yo", "yoh",
  
  # vi. Interrogatives, pronouns and related words
  "another", "anybody", "anyone", "anything", "each other", "everybody", 
  "everyone", "everything", "he", "hell", "her", "herself", "hes", "him", 
  "himself", "his", "how", "i", "id", "i'll", "im", "it", "itself", "i've", 
  "ive", "many", "me", "most", "myself", "no one", "nobody", "none", 
  "one another", "other", "ourselves", "she", "shed", "shes", "some", 
  "somebody", "someone", "something", "such", "that", "themselves", "they", 
  "them", "these","this", "those", "us", "what", "whatever", "whatsoever", 
  "when", "whenever", "where", "wherever", "which", "whichever", "who", 
  "whoever", "whom", "whomever", "whose", "whosever", "why", "you", 
  "yourself", "yourselves",
  
  # vii. Modal, auxiliary and other common verbs
  "be", "been", "being", "bring", "can", "cant", "come", "could", "couldnt", 
  "did", "do", "does", "done", "fill", "filled", "filling", "fills", "full", 
  "fuller", "get", "getting", "go", "going", "gone", "gonna", "got", "gotten", 
  "guess", "had", "has", "have", "having", "held", "hold", "holds", "holding", 
  "left", "leaving", "leave", "made", "make", "makes", "making", "may", "might", 
  "mightnt", "must", "mustnt", "need", "neednt", "ought", "oughtnt", "pull", 
  "push", "pulled", "pushed", "pulls", "pushes", "pulling", "pushing", "sat", 
  "set", "shall", "shant", "should", "shouldnt", "sit", "sits", "start", 
  "starts", "starting", "started", "stand", "stands", "stay", "stop", "wait", 
  "waits", "wanna", "want", "was", "wasnt", "went", "were", "werent", "will", 
  "wont", "would", "wouldnt",
  
  # viii. Number words
  "billion", "double", "eighth", "eighteen", "eighty", "eleven", "eleventh", 
  "fifth", "fifteen", "fifty", "first", "four", "fourteen", "fourth", "few", 
  "hundred", "last", "million", "nineteen", "ninety", "ninth", "none", "one", 
  "second", "seventh", "seventeen", "seventy", "single", "six", "sixteen", 
  "sixth", "ten", "tenth", "third", "thirteen", "thirty", "thousand", "three", 
  "trillion", "triple", "twelfth", "twelve", "twenty", "two", "zero",
  
  # ix. Prepositions
  "about", "above", "across", "after", "against", "along", "amid", "among", 
  "amongst", "anti", "around", "as", "at", "before", "behind", "below", 
  "beneath", "beside", "besides", "between", "beyond", "but", "by", 
  "concerning", "considering", "despite", "down", "during", "except", 
  "excepting", "excluding", "far", "following", "for", "from", "here", "in", 
  "inside", "into", "like", "minus", "near", "of", "off", "on", "onto", 
  "opposite", "outside", "over", "past", "per", "plus", "regarding", "round", 
  "save", "since", "than", "then", "there", "through", "throughout", "to", 
  "toward", "towards", "under", "underneath", "unlike", "until", "up", "upon", 
  "versus", "via", "with", "within", "without", 
  
  # x. Time words
  "afternoon", "annual", "annually", "daily", "day", "hour", "hourly", 
  "minute", "moment", "night", "nightly", "now", "oclock", "today", 
  "tomorrow", "week", "weekly", "year", "yearly", "yesterday",
  
  # xi. Titles and abbreviations
  "co", "dr", "ma'am", "madam", "maam", "mr", "mrs", "ms", "mt", "rd", "sir", 
  "st"
    
  )
```

**Keep words list**

-   Keep words are a list of words to retain by removing them from the default stopwords list contained in the `tidytext` package.

-   A diction of sight is key to this corpus as "eyes" and related terms are highly frequent words. Thus, "see\*" is retained with `keep_words`. Otherwise, the terms are removed by the default stopwords list.

```{r}
keep_words <- c("see", "seeing", "saw", "seen") 
```

**Combine default stopwords with term filter list**

-   A dataframe is created below for `all_stopwords` combining the default stopwords and terms from the `filter_list`, but with `keep_words` excluded from this dataframe.

```{r}
all_stopwords <- unique(c(stop_words$word, filter_list))
all_stopwords <- all_stopwords[!all_stopwords %in% keep_words]
filter_list_df <- data.frame(word = filter_list) 
```

-   If necessary, the `filter_list` can be written to a CSV file for inspection.

```{r}
write_csv(filter_list_df, "ADD FILE PATHWAY/term_filter_list.csv")  # Add your chosen file pathway
```

-   Default stopwords also can be saved for inspection.

```{r}
default_stopwords <- stop_words # tidytext stopword list
write_csv(default_stopwords, "ADD FILE PATHWAY/default_stopwords.csv") 
```

#### STEP 2B: LOWERCASE ALL TERMS AND REMOVE STOPWORDS, PUNCTUATION AND NUMBERS {style="color: green"}

-   Terms saved in `tokens_cleaned` are now lowercased. Stopwords and filter words together (`all_stopwords`), punctuation and digits are then removed. The number of terms now remaining is returned.

```{r}
tokens_cleaned <- tokens %>%
  mutate(word = tolower(word)) %>% 
  filter(str_detect(word, "^[a-z]+$")) %>%
  filter(!word %in% all_stopwords) 
  
View(tokens_cleaned)

dim(tokens_cleaned) 
```

#### STEP 2C: CALCULATE TERM FREQUENCIES AFTER REMOVING STOPWORDS {style="color: green"}

-   After the stopwords are removed but before lemmatising the text, the word frequencies are counted, sorted and saved below.

```{r}
word_frequency <- tokens_cleaned %>%
  count(word, sort = TRUE)

write_csv(word_frequency, "ADD FILE PATHWAY/word_frequency_after_stopwords_removed.csv")
```

#### STEP 2D: LEMMATISE ALL TERMS TO ROOT FORM (STEMMING IS NOT DONE) {style="color: green"}

-   Terms are now lemmatised to their correct root form with the `tidystem` function `lemmatize_words`.
-   They are not stemmed as stemming returns terms that do not take into account words like irregular verbs (e.g. run and ran), leading to results that are difficult to interpret.
-   After lemmatising, the terms are saved again in a separate file for inspection.

```{r}
tokens_cleaned <- tokens_cleaned %>%
  mutate(word = lemmatize_words(word))

View(tokens_cleaned)

write_csv(word_frequency, "ADD FILE PATHWAY/word_frequency_after_lemmatising.csv")
```

#### STEP 2E: CALCULATE TERM FREQUENCIES AFTER LEMMATISING {style="color:green"}

-   After stopwords are removed and the text is lemmatised to the root form, word frequencies are counted, sorted and saved.

```{r}
word_frequency <- tokens_cleaned %>%
  count(word, sort = TRUE)  

write_csv(word_frequency, "ADD FILE PATHWAY/word_frequency_after_lemmatising.csv") 
```

#### STEP 3A: CREATE A DOCUMENT-TERM MATRIX (DTM) {style="color:green"}

-   A DTM is created as this is the correct format for the LDA program to read and work with.

```{r}
dtm <- tokens_cleaned %>% 
  count(doc_id, word) %>%  
  cast_dtm(doc_id, word, n) 

dtm

dtm_df <- as.data.frame(as.matrix(dtm)) 
```

-   The results above show the number of documents (155) and number of unique terms (8 128) so that the DTM is a 155 × 8 128 matrix. Each cell shows the term frequency or how often a word appears in a document.

-   The sparsity (98%) shows the proportion of cells in the matrix that are empty (zero). This is not uncommon in natural language data with stopwords removed in that few words appear across all the documents.

-   98% sparsity indirectly suggests a higher level of uniqueness or variability among the texts in terms of vocabulary. In other words, there is lower lexical overlap, which is to be expected in a corpus of creative literature from different genres and from a variety of authors.

#### STEP 3B: BEGIN TOPIC MODELLING WITH LATENT DIRICHLET ALLOCATION (LDA) {style="color:green"}

-   We are now ready to run the `lda_model`. The code below starts with 10 topics with 10 terms each to test the model. The number of topics is the `k` value. The `k` value will be adjusted later after calculating coherence scores and evaluating the term clusters to determine a suitable number of topics for this corpus.

-   Gibbs sampling is applied as this method provides more distinct topics than the default sampling method.

-   After the terms are derived, they are sorted for each topic alphabetically and then the topics are saved to a file.

```{r}
num_topics <- 10

lda_model <- LDA(dtm, 
                 k = num_topics, 
                 method = "Gibbs",    
                 control = list(seed = 1818)) #This is the random seed I usually use.

topics_terms <- terms(lda_model, 10) 
topics_terms

topics_df <- as.data.frame(topics_terms)
topics_df <- cbind(Topic = rownames(topics_df), topics_df) 

sorted_topics_df <- topics_df %>% 
  mutate(across(-Topic, ~sort(.))) # Sort terms alphabetically 

write_csv(topics_df, "ADD FILE PATHWAY/lda_topics_10.csv") 
```

#### STEP 4A: TEST THE COHERENCE OF DIFFERENT NUMBERS OF TOPICS {style="color:green"}

-   10 topics were derived above to get a sense of the type of results the model will provide. The code below now tests the coherence of different numbers of topics as `k` = 10 is not necessarily the optimum number.

-   Coherence scores are essentially a measure of how often terms co-occur in documents, justifying their being grouped together in a topic.

-   The pairwise approach followed measures term similarity by comparing two terms at a time to each other. The results are then aggregated across all pairs.

-   The code will iterate through a range of `k` values (from 2 to 34 in increments of 2) and determine the coherence scores. The code will take some time to run as it tests all pairwise terms for all the `k` values requested here.

-   The results are displayed in a dataframe and saved as a file for comparison.

```{r}
compute_coherence <- function(dtm, k_values) { 
  coherence_scores <- c() 
  for (k in k_values) {  
    lda_model <- LDA(dtm, k = k, control = list(seed = 1818)) 
    
   top_terms <- terms(lda_model, 10) 
    
    dtm_matrix <- as.matrix(dtm) 
    
    coherence <- sapply(1:ncol(top_terms), function(i) { 
      term_list <- top_terms[, i]
      term_index <- match(term_list, colnames(dtm_matrix))
      term_pairs <- combn(term_index, 2)
      
      pairwise_coherence <- apply(term_pairs, 2, function(pair) {
        term1 <- pair[1]
        term2 <- pair[2]
        term1_freq <- dtm_matrix[, term1]
        term2_freq <- dtm_matrix[, term2]
        sum(term1_freq * term2_freq) / (sum(term1_freq) * sum(term2_freq))
      })
      mean(pairwise_coherence)
    })
    coherence_scores <- c(coherence_scores, mean(coherence))
  }
  return(data.frame(k = k_values, coherence = coherence_scores))
}

k_values <- seq(2, 34, by = 2)  
coherence_results <- compute_coherence(dtm, k_values)
coherence_results

write_csv(coherence_results, "ADD FILE PATHWAY/data_output/coherence_scores.csv") 
```

#### STEP 4B: CREATE LINE GRAPH SHOWING COHERENCE SCORES {style="color:green"}

-   Coherence scores can be plotted on a line graph for easier interpretability. The code below creates one with `ggplot`.

```{r}
line_graph_coherence_scores <- ggplot(coherence_results, aes(x = k, y = coherence)) +
  geom_line(color = "darkorange") +
  geom_point(color = "darkorange") +
  labs(title = "Optimal number of topics (coherence score)",
       x = "Number of topics (k)",
       y = "Coherence score") +
  scale_x_continuous(breaks = seq(min(coherence_results$k), max(coherence_results$k), by = 2)) + # Ticks on the x-axis
  scale_y_continuous(n.breaks = 15) + # Ticks on the y-axis
  theme_bw()

line_graph_coherence_scores

ggsave("ADD FILE PATHWAY/line_graph_coherence_scores.png", plot = line_graph_coherence_scores, width = 10, height = 6) 

```

#### STEP 5A: RE-RUN TOPIC MODELLING FOR 30 TOPICS {style="color:green"}

-   The **acceptable range for coherence scores** is **0.04 to 0.07** for LDA. The data indicated on the line graph shows that the maximum number of coherent topics is 30, with a score of \~0.059, before starting to drop again. The model is thus rerun for 30 topics below.

-   While the scores are useful to test the general coherence of the topics, they are mathematically derived by the computer, which does not have an understanding of semantics and context. Thus, the researcher will need to apply their judgement in determining which set of results will be most useful and practical to include in the analysis, as will be discussed further below.

```{r}
num_topics <- 30 

lda_model <- LDA(dtm, k = num_topics, control = list(seed = 1818))

topics_terms <- terms(lda_model, 10)  
topics_terms

topics_df <- as.data.frame(topics_terms)
topics_df <- cbind(Topic = rownames(topics_df), topics_df) 

sorted_topics_df <- topics_df %>% # Sort terms alphabetically 
  mutate(across(-Topic, ~sort(.)))

write_csv(topics_df, "ADD FILE PATHWAY/lda_topics_30.csv") 
```

**Evaluation of the results**

-   The coherence results and graph suggest that 30 topics are optimal, with a score of \~0.059. This is a mathematically derived number and can be useful.

<!-- -->

-   However, the researcher applied her judgement at this stage and previous research using topic modelling for literary analysis. When words are repeated too often (more than three or four times in this case) across multiple topics, the results start to become too diffuse and less useful ('overcooked') (Van Kessel, 2019; Weston et al., 2023; Wisdom, 2017). This relates to the higher numbers of topics, especially from 16 and above.

-   Practical constraints also need to be considered for this research. It is impractical to present a high number of topics in the dissertation. Topic modelling forms just one part of the quantitative portion of the study and too many topics will lead to a lengthy discussion. This issue is the case for the higher numbers of topics, especially 14 or more.

-   2 to 10 topics have relatively low coherence scores under \~0.046, which is less desirable. After examining the terms carefully, it was determined that they are also too few and too narrow ('undercooked') in scope for the research purposes and goals compared to 12 topics.

-   Based on the coherence results and the researcher's judgement on repeated words and the scope of the topics, the final number of topics selected is 12, which has a score of \~0.0473. Thus, the code is run again below for 12 topics and these results will be used for further analysis in the dissertation.

    #### STEP 5B: RE-RUN TOPIC MODELLING FOR 12 TOPICS {style="color:green"}

    ```{r}
    num_topics <- 12 

    lda_model <- LDA(dtm, 
                     k = num_topics, 
                     method = "Gibbs",    
                     control = list(seed = 1818))

    topics_terms <- terms(lda_model, 10)  
    topics_terms

    topics_df <- as.data.frame(topics_terms)
    topics_df <- cbind(Topic = rownames(topics_df), topics_df) 

    sorted_topics_df <- topics_df %>% # Sort terms alphabetically 
      mutate(across(-Topic, ~sort(.)))

    write_csv(sorted_topics_df, "ADD FILE PATHWAY/lda_topics_12.csv")
    ```

    #### STEP 6A: EXTRACT BETA VALUES, LIKELY TOPIC FOR EACH TEXT, AND HOW MANY TEXTS BELONG TO EACH TOPIC {style="color:green"}

-   The code below can be run to extract specific results from the LDA model. It first retrieves the beta matrix, which represents the distribution of words across topics and converts it into a dataframe.

-   The **topic each document is most closely associated with** is then determined. These results are of interest to the researcher, but will not be presented in the dissertation.

```{r}
beta_matrix <- posterior(lda_model)$terms  
beta_df <- as.data.frame(beta_matrix)

document_topics <- topics(lda_model) 

document_topics_df <- data.frame(
  doc_id = names(document_topics),
  topic = as.integer(document_topics)
)

write_csv(document_topics_df, "ADD FILE PATHWAY/documents_by_topic.csv")
```

-   The next piece of code extracts the **number of documents that each topic is associated**, indicating its relative strength in the corpus.

```{r}
topic_counts <- data.frame(topic = document_topics) %>%
  group_by(topic) %>%
  summarise(count = n())  

write_csv(topic_counts, "ADD FILE PATHWAY/topic_counts.csv") 
```

#### STEP 6B: EXTRACT GAMMA VALUES AND CREATE A TOPIC HEATMAP {style="color:green"}

-   This code below visualises the document-topic relationships with a heatmap. It first extracts the topic probabilities (gamma values) and then restructures, coverts and labels the data.

-   The 25 most topic-representative documents are selected and a heatmap showing each document's dominant topics is produced. The heatmap is saved as an image file.

```{r}
# Extract topic proportions for each document. Rows = documents and columns = topics

lda_gamma <- posterior(lda_model)$topics  

# Create an lda_gamma dataframe

lda_gamma_df <- as.data.frame(lda_gamma) %>%   
  mutate(doc_id = rownames(.)) %>%   # Preserve the correct document order
  mutate(doc_id_numeric = as.numeric(gsub("^(\\d+).*", "\\1", doc_id))) %>%  # Capture numbering in the document ID and convert from character string to numeric format
  arrange(doc_id_numeric) %>% 
  mutate(Document = paste0("Doc_", seq_len(nrow(.)))) %>% 
  select(-doc_id, -doc_id_numeric)

# Convert the dataframe to long format

lda_gamma_long <- lda_gamma_df %>%   
  pivot_longer(
    cols = -Document,
    names_to = "Topic",
    values_to = "Proportion"  # All columns except "Document" will be transposed/re-stacked.
  )

# Identify the top most representative documents    

top_25_docs <- lda_gamma_long %>% 
  group_by(Document) %>%
  summarise(Max_Prob = max(Proportion)) %>%
  ungroup() %>%
  slice_max(Max_Prob, n = 25) %>%  
  pull(Document)

# Filter using the preserved document IDs
lda_filtered <- lda_gamma_long %>%  
  filter(Document %in% top_25_docs) %>% 
  mutate(Topic = factor(Topic, levels = mixedsort(unique(Topic))))

# Create the heatmap

heatmap <- ggplot(lda_filtered, aes(x = factor(Document, levels = unique(Document)), 
                                    y = Topic, 
                                    fill = Proportion)) +
  geom_tile() +
  coord_fixed() +
  scale_x_discrete(labels = function(x) gsub("Doc_", "Text  #", x)) +
   scale_fill_gradient(low = "gold", high = "firebrick") +
  labs(x = "Document", title = "Top 25 documents by maximum topic probability") +
  theme_bw() +
  theme(
    axis.text.x = element_text(angle = 60, hjust = 1, size = 8), #Titled x axis lables
    legend.position = "none"  # Legend is excluded
  )
    
heatmap

ggsave("ADD FILE PATHWAY/heatmap_top_25.png", plot = heatmap, width = 20, height = 9)
```

#### **SOURCES** {style="color: green"}

Ajinaja, M. O., Adetunmbi, A. O., Ugwu, C. C., & Popoola, O. S. (2023). Semantic similarity measure for topic modeling using latent Dirichlet allocation and collapsed Gibbs sampling. *Iran Journal of Computer Science*, 6(1), pp. 81–94. <http://dx.doi.org/10.1007/s42044-022-00124-7>

Blei, D.M., Ng, A.Y., & Jordan, M.I. (2003). Latent Dirichlet allocation. *Journal of Machine Learning Research*, 3(Jan), pp. 993–1022. <https://dl.acm.org/doi/10.5555/944919.944937>

Casella, G., & George, E. I. (1992). Explaining the Gibbs sampler. *The American Statistician*, 46(3), pp. 167–174. <https://doi.org/10.1080/00031305.1992.10475878>

Essberger, J. (2006). English Prepositions Listed. <https://www.vocabineer.com/wp-content/uploads/2019/01/150-English-Prepositions.pdf>

Fox, C. (1989). A stop list for general text. *Acm Sigir Forum*, 24(1–2), pp. 19–21. <https://doi.org/10.1145/378881.378888>

Grün, B., & Hornik, K. (2024). Package ‘topicmodels’. CRAN Repository. <https://cran.radicaldevelop.com/web/packages/topicmodels/topicmodels.pdf?utm_source=textcortex&utm_medium=zenochat>

Hellín, C.J., Valledor, A., Cuadrado-Gallego, J.J., Tayebi, A., & Gómez, J. (2023). A Comparative Study on R Packages for Text Mining*. IEEE Access*, 11, 99083–99100. <https://doi.org/10.1109/access.2023.3310818>

Omar, A. (2020). On the digital applications in the thematic literature studies of Emily Dickinson’s poetry. *International Journal of Advanced Computer Science and Applications*, 11(6): 361–365. <https://dx.doi.org/10.14569/IJACSA.2020.0110647>

R Charts. (n.d.). Heat map in ggplot2. <https://r-charts.com/correlation/heat-map-ggplot2>

Rha, L., & Silver, S. (2021). Topic Modeling and Analysis: Comparing the Most Common Topics in 19th-Century Novels Written by Female Writers. *Aresty Rutgers Undergraduate Research Journal*, 1(3), pp. 1–8. <https://doi.org/10.14713/arestyrurj.v1i3.172>

Rinker, T. W. (2018). textstem: Tools for stemming and lemmatizing text. Version 0.1.4. <http://github.com/trinker/textstem>

Silge, J. & Robinson, D. (2017). *Text Mining with R.* Sebastopol, CA: O’Reilly Media. <https://www.tidytextmining.com>

Van Kessel, P. (2019). Overcoming the limitations of topic models with a semi-supervised approach. Pew Research Centre. <https://www.pewresearch.org/decoded/2019/04/10/overcoming-the-limitations-of-topic-models-with-a-semi-supervised-approach>

Weston, S. J., Shryock, I., Light, R., & Fisher, P. A. (2023). Selecting the Number and Labels of Topics in Topic Modeling: A Tutorial. *Advances in Methods and Practices in Psychological Science*, 6(2). <https://doi.org/10.1177/25152459231160105>

Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., et al. (2019). Welcome to the tidyverse. *Journal of Open Source Software*, 4(43), 1686. [https://doi.org/10.21105/joss.01686](#0){.uri}

Wisdom, A. (2017). Topic Modeling: Optimizing for Human Interpretability. <https://developer.squareup.com/blog/topic-modeling-optimizing-for-human-interpretability>
